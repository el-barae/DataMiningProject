# √âtude Data Mining - Syst√®me de Recommandation Candidat-Emploi
# ================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, silhouette_score
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
import warnings

warnings.filterwarnings('ignore')

# Configuration des graphiques
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_palette("husl")

print("=" * 60)
print("√âTUDE DATA MINING - SYST√àME DE RECOMMANDATION CANDIDAT-EMPLOI")
print("=" * 60)

# =====================================================
# 1. CHARGEMENT ET EXPLORATION INITIALE DES DONN√âES
# =====================================================

print("\n1. CHARGEMENT ET EXPLORATION INITIALE")
print("-" * 40)

# Chargement des donn√©es
df = pd.read_csv("candidate_job_match_dataset.csv")

print(f"Dimensions du dataset: {df.shape}")
print(f"\nPremi√®res lignes:")
print(df.head())

print(f"\nInformations g√©n√©rales:")
print(df.info())

print(f"\nStatistiques descriptives:")
print(df.describe())

# =====================================================
# 2. NETTOYAGE ET PR√âPARATION DES DONN√âES
# =====================================================

print("\n\n2. NETTOYAGE ET PR√âPARATION DES DONN√âES")
print("-" * 45)

# V√©rification des valeurs manquantes
print("Valeurs manquantes par colonne:")
missing_values = df.isnull().sum()
print(missing_values)

# V√©rification des doublons
duplicates = df.duplicated().sum()
print(f"\nNombre de doublons: {duplicates}")

# Suppression des doublons si n√©cessaire
if duplicates > 0:
    df = df.drop_duplicates()
    print(f"Doublons supprim√©s. Nouvelles dimensions: {df.shape}")

# Traitement des valeurs manquantes
df_clean = df.copy()

# Pour les colonnes num√©riques, remplacer par la m√©diane
numeric_cols = ['ExperienceYears', 'RequiredExperience', 'MatchScore']
for col in numeric_cols:
    if df_clean[col].isnull().sum() > 0:
        median_val = df_clean[col].median()
        df_clean[col].fillna(median_val, inplace=True)
        print(f"Valeurs manquantes dans {col} remplac√©es par la m√©diane: {median_val}")

# Pour les colonnes cat√©gorielles, remplacer par le mode
categorical_cols = ['CandidateName', 'EducationLevel', 'Skills', 'RequiredSkills',
                    'JobTitle', 'ClientName', 'EducationRequired']
for col in categorical_cols:
    if df_clean[col].isnull().sum() > 0:
        mode_val = df_clean[col].mode()[0]
        df_clean[col].fillna(mode_val, inplace=True)
        print(f"Valeurs manquantes dans {col} remplac√©es par le mode: {mode_val}")

# D√©tection et traitement des valeurs aberrantes
print(f"\nD√©tection des valeurs aberrantes:")
for col in numeric_cols:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = df_clean[(df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)][col]
    print(f"{col}: {len(outliers)} valeurs aberrantes d√©tect√©es")

    # Limitation des valeurs aberrantes (winsorization)
    df_clean[col] = np.clip(df_clean[col], lower_bound, upper_bound)

print(f"\nDonn√©es nettoy√©es. Dimensions finales: {df_clean.shape}")

# =====================================================
# 3. ANALYSE EXPLORATOIRE DES DONN√âES (EDA)
# =====================================================

print("\n\n3. ANALYSE EXPLORATOIRE DES DONN√âES")
print("-" * 40)

# Distribution des variables num√©riques
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Distribution des Variables Num√©riques', fontsize=16, fontweight='bold')

df_clean['ExperienceYears'].hist(bins=20, ax=axes[0, 0], alpha=0.7, color='skyblue')
axes[0, 0].set_title('Distribution Exp√©rience Candidats')
axes[0, 0].set_xlabel('Ann√©es d\'exp√©rience')

df_clean['RequiredExperience'].hist(bins=20, ax=axes[0, 1], alpha=0.7, color='lightcoral')
axes[0, 1].set_title('Distribution Exp√©rience Requise')
axes[0, 1].set_xlabel('Ann√©es d\'exp√©rience requise')

df_clean['MatchScore'].hist(bins=20, ax=axes[1, 0], alpha=0.7, color='lightgreen')
axes[1, 0].set_title('Distribution Score de Match')
axes[1, 0].set_xlabel('Score de correspondance')

# Boxplot pour d√©tecter les outliers
df_clean[['ExperienceYears', 'RequiredExperience', 'MatchScore']].boxplot(ax=axes[1, 1])
axes[1, 1].set_title('Boxplot - D√©tection Outliers')

plt.tight_layout()
plt.show()

# Analyse des variables cat√©gorielles
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Distribution des Variables Cat√©gorielles', fontsize=16, fontweight='bold')

# Top 10 des niveaux d'√©ducation
education_counts = df_clean['EducationLevel'].value_counts().head(10)
education_counts.plot(kind='bar', ax=axes[0, 0], color='lightblue')
axes[0, 0].set_title('Top 10 Niveaux d\'√âducation Candidats')
axes[0, 0].tick_params(axis='x', rotation=45)

# Top 10 des niveaux d'√©ducation requis
edu_req_counts = df_clean['EducationRequired'].value_counts().head(10)
edu_req_counts.plot(kind='bar', ax=axes[0, 1], color='lightcoral')
axes[0, 1].set_title('Top 10 Niveaux d\'√âducation Requis')
axes[0, 1].tick_params(axis='x', rotation=45)

# Top 10 des titres de poste
job_counts = df_clean['JobTitle'].value_counts().head(10)
job_counts.plot(kind='bar', ax=axes[1, 0], color='lightgreen')
axes[1, 0].set_title('Top 10 Titres de Poste')
axes[1, 0].tick_params(axis='x', rotation=45)

# Top 10 des clients
client_counts = df_clean['ClientName'].value_counts().head(10)
client_counts.plot(kind='bar', ax=axes[1, 1], color='gold')
axes[1, 1].set_title('Top 10 Clients')
axes[1, 1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# Matrice de corr√©lation
print("\nMatrice de corr√©lation des variables num√©riques:")
correlation_matrix = df_clean[['ExperienceYears', 'RequiredExperience', 'MatchScore']].corr()
print(correlation_matrix)

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
            square=True, cbar_kws={'shrink': 0.8})
plt.title('Matrice de Corr√©lation des Variables Num√©riques', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Analyse de la relation exp√©rience vs match score
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(df_clean['ExperienceYears'], df_clean['MatchScore'], alpha=0.6, color='blue')
plt.xlabel('Exp√©rience Candidat (ann√©es)')
plt.ylabel('Score de Match')
plt.title('Exp√©rience Candidat vs Score de Match')

plt.subplot(1, 2, 2)
plt.scatter(df_clean['RequiredExperience'], df_clean['MatchScore'], alpha=0.6, color='red')
plt.xlabel('Exp√©rience Requise (ann√©es)')
plt.ylabel('Score de Match')
plt.title('Exp√©rience Requise vs Score de Match')

plt.tight_layout()
plt.show()

# =====================================================
# 4. FEATURE ENGINEERING
# =====================================================

print("\n\n4. FEATURE ENGINEERING")
print("-" * 25)

# Cr√©ation de nouvelles variables
df_features = df_clean.copy()

# 1. Diff√©rence d'exp√©rience (candidat - requis)
df_features['ExperienceDiff'] = df_features['ExperienceYears'] - df_features['RequiredExperience']


# 2. Cat√©gorisation du niveau d'exp√©rience
def categorize_experience(years):
    if years <= 2:
        return 'Junior'
    elif years <= 5:
        return 'Mid-level'
    elif years <= 10:
        return 'Senior'
    else:
        return 'Expert'


df_features['ExperienceCategory'] = df_features['ExperienceYears'].apply(categorize_experience)
df_features['RequiredExperienceCategory'] = df_features['RequiredExperience'].apply(categorize_experience)


# 3. Cat√©gorisation du match score - Ajustement des seuils
def categorize_match(score):
    # Ajustement des seuils bas√© sur les quartiles pour assurer une distribution √©quilibr√©e
    q1 = df_clean['MatchScore'].quantile(0.33)
    q2 = df_clean['MatchScore'].quantile(0.67)

    if score <= q1:
        return 'Faible'
    elif score <= q2:
        return 'Moyen'
    else:
        return '√âlev√©'


df_features['MatchCategory'] = df_features['MatchScore'].apply(categorize_match)

# V√©rification de la distribution des classes
print(f"Distribution des classes MatchCategory:")
print(df_features['MatchCategory'].value_counts())
print(f"Nombre de classes uniques: {df_features['MatchCategory'].nunique()}")

# 4. Encodage des variables cat√©gorielles
label_encoders = {}
categorical_columns = ['EducationLevel', 'EducationRequired', 'JobTitle', 'ClientName',
                       'ExperienceCategory', 'RequiredExperienceCategory']

for col in categorical_columns:
    le = LabelEncoder()
    df_features[f'{col}_encoded'] = le.fit_transform(df_features[col].astype(str))
    label_encoders[col] = le

# 5. Normalisation des variables num√©riques
scaler = StandardScaler()
numeric_features = ['ExperienceYears', 'RequiredExperience', 'MatchScore', 'ExperienceDiff']
df_features[numeric_features] = scaler.fit_transform(df_features[numeric_features])

print("Nouvelles variables cr√©√©es:")
print("- ExperienceDiff: Diff√©rence d'exp√©rience (candidat - requis)")
print("- ExperienceCategory: Cat√©gorie d'exp√©rience du candidat")
print("- RequiredExperienceCategory: Cat√©gorie d'exp√©rience requise")
print("- MatchCategory: Cat√©gorie de score de match")
print("- Variables encod√©es pour les colonnes cat√©gorielles")
print("- Variables num√©riques normalis√©es")

# =====================================================
# 5. R√àGLES D'ASSOCIATION (APRIORI)
# =====================================================

print("\n\n5. ANALYSE DES R√àGLES D'ASSOCIATION")
print("-" * 35)

# Pr√©paration des donn√©es pour l'analyse des r√®gles d'association
# Cr√©ation de transactions bas√©es sur les caract√©ristiques des candidats et jobs

transactions = []
for _, row in df_clean.iterrows():
    transaction = [
        f"Education_{row['EducationLevel']}",
        f"JobTitle_{row['JobTitle']}",
        f"EducationReq_{row['EducationRequired']}",
        f"ExpCat_{categorize_experience(row['ExperienceYears'])}",
        f"ExpReqCat_{categorize_experience(row['RequiredExperience'])}",
        f"MatchCat_{categorize_match(row['MatchScore'])}"
    ]
    transactions.append(transaction)

# Encodage des transactions
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_transactions = pd.DataFrame(te_ary, columns=te.columns_)

# Application de l'algorithme Apriori
frequent_itemsets = apriori(df_transactions, min_support=0.1, use_colnames=True)
print(f"Nombre d'itemsets fr√©quents trouv√©s: {len(frequent_itemsets)}")

if len(frequent_itemsets) > 0:
    # G√©n√©ration des r√®gles d'association
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)

    if len(rules) > 0:
        print(f"Nombre de r√®gles d'association g√©n√©r√©es: {len(rules)}")
        print("\nTop 10 des r√®gles d'association (par confiance):")
        top_rules = rules.nlargest(10, 'confidence')[['antecedents', 'consequents', 'support', 'confidence', 'lift']]
        for idx, rule in top_rules.iterrows():
            antecedents = ', '.join(list(rule['antecedents']))
            consequents = ', '.join(list(rule['consequents']))
            print(f"Si {antecedents} => Alors {consequents}")
            print(f"  Support: {rule['support']:.3f}, Confiance: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}\n")
    else:
        print("Aucune r√®gle d'association trouv√©e avec les seuils d√©finis.")
else:
    print("Aucun itemset fr√©quent trouv√©. R√©duction du seuil de support...")
    frequent_itemsets = apriori(df_transactions, min_support=0.05, use_colnames=True)
    print(f"Nombre d'itemsets fr√©quents avec support r√©duit: {len(frequent_itemsets)}")

# =====================================================
# 6. CLUSTERING (K-MEANS ET DBSCAN)
# =====================================================

print("\n\n6. ANALYSE DE CLUSTERING")
print("-" * 25)

# Pr√©paration des donn√©es pour le clustering
clustering_features = ['ExperienceYears', 'RequiredExperience', 'MatchScore', 'ExperienceDiff']
X_cluster = df_features[clustering_features + [col + '_encoded' for col in categorical_columns]]

# K-Means Clustering
print("A. K-MEANS CLUSTERING")
print("-" * 20)

# D√©termination du nombre optimal de clusters (m√©thode du coude)
inertias = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_cluster)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_cluster, cluster_labels))

# Visualisation de la m√©thode du coude et silhouette score
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

ax1.plot(k_range, inertias, marker='o', linewidth=2, markersize=8)
ax1.set_xlabel('Nombre de clusters (k)')
ax1.set_ylabel('Inertie')
ax1.set_title('M√©thode du Coude - K-Means')
ax1.grid(True, alpha=0.3)

ax2.plot(k_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='orange')
ax2.set_xlabel('Nombre de clusters (k)')
ax2.set_ylabel('Score de Silhouette')
ax2.set_title('Score de Silhouette - K-Means')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Choix du nombre optimal de clusters
optimal_k = k_range[np.argmax(silhouette_scores)]
print(f"Nombre optimal de clusters (bas√© sur silhouette score): {optimal_k}")

# Application du K-Means avec le nombre optimal de clusters
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df_features['KMeans_Cluster'] = kmeans_final.fit_predict(X_cluster)

print(f"Score de silhouette final: {silhouette_score(X_cluster, df_features['KMeans_Cluster']):.3f}")

# Analyse des clusters
print("\nAnalyse des clusters K-Means:")
cluster_analysis = df_clean.groupby(df_features['KMeans_Cluster']).agg({
    'ExperienceYears': ['mean', 'std'],
    'RequiredExperience': ['mean', 'std'],
    'MatchScore': ['mean', 'std'],
    'EducationLevel': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'N/A',
    'JobTitle': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'N/A'
}).round(2)

print(cluster_analysis)

# DBSCAN Clustering
print("\nB. DBSCAN CLUSTERING")
print("-" * 20)

# Test de diff√©rents param√®tres pour DBSCAN
eps_values = [0.5, 1.0, 1.5, 2.0]
min_samples_values = [5, 10, 15]

best_silhouette = -1
best_params = {}

for eps in eps_values:
    for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        cluster_labels = dbscan.fit_predict(X_cluster)

        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)

        if n_clusters > 1:  # Au moins 2 clusters pour calculer silhouette
            silhouette_avg = silhouette_score(X_cluster, cluster_labels)
            if silhouette_avg > best_silhouette:
                best_silhouette = silhouette_avg
                best_params = {'eps': eps, 'min_samples': min_samples, 'n_clusters': n_clusters, 'n_noise': n_noise}

if best_params:
    print(f"Meilleurs param√®tres DBSCAN: eps={best_params['eps']}, min_samples={best_params['min_samples']}")
    print(f"Nombre de clusters: {best_params['n_clusters']}, Points de bruit: {best_params['n_noise']}")
    print(f"Score de silhouette: {best_silhouette:.3f}")

    # Application finale de DBSCAN
    dbscan_final = DBSCAN(eps=best_params['eps'], min_samples=best_params['min_samples'])
    df_features['DBSCAN_Cluster'] = dbscan_final.fit_predict(X_cluster)
else:
    print("Aucune configuration DBSCAN satisfaisante trouv√©e.")

# Visualisation des clusters (2D projection)
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
scatter = plt.scatter(df_clean['ExperienceYears'], df_clean['MatchScore'],
                      c=df_features['KMeans_Cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Exp√©rience Candidat')
plt.ylabel('Score de Match')
plt.title('Clusters K-Means')
plt.colorbar(scatter)

if 'DBSCAN_Cluster' in df_features.columns:
    plt.subplot(1, 3, 2)
    scatter = plt.scatter(df_clean['ExperienceYears'], df_clean['MatchScore'],
                          c=df_features['DBSCAN_Cluster'], cmap='viridis', alpha=0.6)
    plt.xlabel('Exp√©rience Candidat')
    plt.ylabel('Score de Match')
    plt.title('Clusters DBSCAN')
    plt.colorbar(scatter)

plt.subplot(1, 3, 3)
scatter = plt.scatter(df_clean['RequiredExperience'], df_clean['MatchScore'],
                      c=df_features['KMeans_Cluster'], cmap='viridis', alpha=0.6)
plt.xlabel('Exp√©rience Requise')
plt.ylabel('Score de Match')
plt.title('K-Means: Exp. Requise vs Match')
plt.colorbar(scatter)

plt.tight_layout()
plt.show()

# =====================================================
# 7. CLASSIFICATION (PR√âDICTION DU NIVEAU DE MATCH)
# =====================================================

print("\n\n7. MOD√àLES DE CLASSIFICATION")
print("-" * 30)

# Pr√©paration des donn√©es pour la classification
# Variable cible: MatchCategory (Faible, Moyen, √âlev√©)
X = df_features[['ExperienceYears', 'RequiredExperience', 'ExperienceDiff'] +
                [col + '_encoded' for col in categorical_columns]]
y = df_features['MatchCategory']

# Encodage de la variable cible
le_target = LabelEncoder()
y_encoded = le_target.fit_transform(y)

# Division des donn√©es avec v√©rification des classes
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3,
                                                    random_state=42, stratify=y_encoded)

print(f"Taille de l'ensemble d'entra√Ænement: {X_train.shape}")
print(f"Taille de l'ensemble de test: {X_test.shape}")
print(f"Distribution des classes dans y:")
print(pd.Series(y).value_counts())
print(f"Distribution des classes encod√©es:")
print(pd.Series(y_encoded).value_counts())

# V√©rification que nous avons au moins 2 classes
n_classes = len(np.unique(y_encoded))
print(f"Nombre de classes uniques: {n_classes}")

if n_classes < 2:
    print("‚ö†Ô∏è ATTENTION: Moins de 2 classes d√©tect√©es. Ajustement des seuils de cat√©gorisation...")

    # Recat√©gorisation avec des seuils plus fins
    score_min = df_clean['MatchScore'].min()
    score_max = df_clean['MatchScore'].max()
    score_range = score_max - score_min


    def categorize_match_fine(score):
        threshold1 = score_min + score_range * 0.4
        threshold2 = score_min + score_range * 0.7

        if score <= threshold1:
            return 'Faible'
        elif score <= threshold2:
            return 'Moyen'
        else:
            return '√âlev√©'


    # R√©application de la cat√©gorisation
    df_features['MatchCategory'] = df_clean['MatchScore'].apply(categorize_match_fine)
    y = df_features['MatchCategory']
    y_encoded = le_target.fit_transform(y)

    print(f"Nouvelle distribution apr√®s ajustement:")
    print(pd.Series(y).value_counts())

    # Nouvelle division des donn√©es
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3,
                                                        random_state=42, stratify=y_encoded)

# Mod√®les √† tester - avec gestion des cas mono-classe
models = {}

# V√©rification du nombre de classes avant d'ajouter les mod√®les
if len(np.unique(y_encoded)) >= 2:
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'K-NN': KNeighborsClassifier(n_neighbors=min(5, len(np.unique(y_encoded)))),
        'SVM': SVC(random_state=42, probability=True)  # Ajout de probability=True pour predict_proba
    }
else:
    print("‚ùå Impossible de faire de la classification avec une seule classe.")
    print("Cr√©ation de classes artificielles bas√©es sur les valeurs num√©riques...")

    # Cr√©ation de classes bas√©es sur les quantiles des scores originaux
    df_features['MatchCategory'] = pd.qcut(df_clean['MatchScore'],
                                           q=3,
                                           labels=['Faible', 'Moyen', '√âlev√©'])
    y = df_features['MatchCategory']
    y_encoded = le_target.fit_transform(y)

    # Nouvelle division
    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3,
                                                        random_state=42, stratify=y_encoded)

    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        'Decision Tree': DecisionTreeClassifier(random_state=42),
        'K-NN': KNeighborsClassifier(n_neighbors=min(5, len(np.unique(y_encoded)))),
        'SVM': SVC(random_state=42, probability=True)
    }

    print(f"Nouvelles classes cr√©√©es:")
    print(pd.Series(y).value_counts())

# Entra√Ænement et √©valuation des mod√®les
results = {}

print("\nR√©sultats des mod√®les de classification:")
print("=" * 50)

for name, model in models.items():
    print(f"\n{name.upper()}")
    print("-" * len(name))

    # Entra√Ænement
    model.fit(X_train, y_train)

    # Pr√©dictions
    y_pred = model.predict(X_test)

    # M√©triques
    accuracy = accuracy_score(y_test, y_pred)
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)

    results[name] = {
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'model': model
    }

    print(f"Pr√©cision sur test: {accuracy:.3f}")
    print(f"Validation crois√©e: {cv_scores.mean():.3f} (¬±{cv_scores.std():.3f})")

    # Rapport de classification d√©taill√©
    print(f"\nRapport de classification:")
    print(classification_report(y_test, y_pred,
                                target_names=le_target.classes_))

# Comparaison des mod√®les
print("\n\nCOMPARAISON DES MOD√àLES")
print("=" * 25)

comparison_df = pd.DataFrame({
    'Mod√®le': list(results.keys()),
    'Pr√©cision Test': [results[model]['accuracy'] for model in results.keys()],
    'CV Moyenne': [results[model]['cv_mean'] for model in results.keys()],
    'CV √âcart-type': [results[model]['cv_std'] for model in results.keys()]
})

print(comparison_df.round(3))

# Visualisation des performances
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
model_names = list(results.keys())
accuracies = [results[model]['accuracy'] for model in model_names]
colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']
bars = plt.bar(model_names, accuracies, color=colors)
plt.ylabel('Pr√©cision')
plt.title('Pr√©cision sur Ensemble de Test')
plt.ylim(0, 1)
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
             f'{acc:.3f}', ha='center', va='bottom')

plt.subplot(1, 2, 2)
cv_means = [results[model]['cv_mean'] for model in model_names]
cv_stds = [results[model]['cv_std'] for model in model_names]
bars = plt.bar(model_names, cv_means, yerr=cv_stds, capsize=5, color=colors)
plt.ylabel('Pr√©cision (Validation Crois√©e)')
plt.title('Performance en Validation Crois√©e')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

# Analyse d'importance des features (Random Forest)
if 'Random Forest' in results:
    rf_model = results['Random Forest']['model']
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf_model.feature_importances_
    }).sort_values('Importance', ascending=False)

    print(f"\nIMPORTANCE DES VARIABLES (Random Forest):")
    print("-" * 45)
    print(feature_importance.head(10))

    # Visualisation de l'importance des features
    plt.figure(figsize=(10, 6))
    top_features = feature_importance.head(10)
    plt.barh(top_features['Feature'], top_features['Importance'])
    plt.xlabel('Importance')
    plt.title('Top 10 des Variables les Plus Importantes')
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.show()

# =====================================================
# 8. SYST√àME DE RECOMMANDATION
# =====================================================

print("\n\n8. SYST√àME DE RECOMMANDATION")
print("-" * 30)

# Fonction de recommandation bas√©e sur le meilleur mod√®le
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']

print(f"Utilisation du meilleur mod√®le: {best_model_name}")


def recommend_candidates(job_requirements, top_n=5):
    """
    Recommande les meilleurs candidats pour un poste donn√©
    """
    # Simulation d'une base de candidats (utilisation du dataset existant)
    candidates_df = df_clean.copy()

    # Calcul du score de correspondance pour chaque candidat
    recommendations = []

    for idx, candidate in candidates_df.iterrows():
        # Cr√©ation du vecteur de features pour ce candidat
        candidate_features = [
            (candidate['ExperienceYears'] - scaler.mean_[0]) / scaler.scale_[0],  # Normalisation
            (job_requirements['required_experience'] - scaler.mean_[1]) / scaler.scale_[1],
            (candidate['ExperienceYears'] - job_requirements['required_experience'] - scaler.mean_[3]) / scaler.scale_[
                3]
        ]

        # Ajout des features encod√©es (simplification)
        for col in categorical_columns:
            if col == 'EducationRequired':
                # Utilisation de l'√©ducation requise du job
                if job_requirements['education'] in label_encoders[col].classes_:
                    encoded_val = label_encoders[col].transform([job_requirements['education']])[0]
                else:
                    encoded_val = 0
            elif col == 'JobTitle':
                if job_requirements['job_title'] in label_encoders[col].classes_:
                    encoded_val = label_encoders[col].transform([job_requirements['job_title']])[0]
                else:
                    encoded_val = 0
            else:
                # Pour les autres colonnes, utiliser les valeurs du candidat
                if candidate[col.replace('_encoded', '')] in label_encoders[col].classes_:
                    encoded_val = label_encoders[col].transform([str(candidate[col.replace('_encoded', '')])])[0]
                else:
                    encoded_val = 0
            candidate_features.append(encoded_val)

        # Pr√©diction du niveau de match
        try:
            match_prediction = best_model.predict([candidate_features])[0]

            # V√©rification si le mod√®le a predict_proba
            if hasattr(best_model, 'predict_proba'):
                match_prob = best_model.predict_proba([candidate_features])[0].max()
            else:
                # Pour les mod√®les sans predict_proba, utiliser une confiance bas√©e sur la distance
                match_prob = 0.8  # Valeur par d√©faut

            recommendations.append({
                'candidate_name': candidate['CandidateName'],
                'experience': candidate['ExperienceYears'],
                'education': candidate['EducationLevel'],
                'skills': candidate['Skills'],
                'predicted_match': le_target.inverse_transform([match_prediction])[0],
                'confidence': match_prob,
                'actual_match_score': candidate['MatchScore']
            })
        except Exception as e:
            # En cas d'erreur, utiliser le score actuel
            recommendations.append({
                'candidate_name': candidate['CandidateName'],
                'experience': candidate['ExperienceYears'],
                'education': candidate['EducationLevel'],
                'skills': candidate['Skills'],
                'predicted_match': categorize_match_fine(
                    candidate['MatchScore']) if 'categorize_match_fine' in locals() else categorize_match(
                    candidate['MatchScore']),
                'confidence': candidate['MatchScore'],
                'actual_match_score': candidate['MatchScore']
            })

    # Tri des recommandations par niveau de match et confiance
    recommendations_df = pd.DataFrame(recommendations)

    # Ordre de priorit√©: √âlev√© > Moyen > Faible
    match_priority = {'√âlev√©': 3, 'Moyen': 2, 'Faible': 1}
    recommendations_df['match_priority'] = recommendations_df['predicted_match'].map(match_priority)

    # Tri par priorit√© puis par confiance
    recommendations_df = recommendations_df.sort_values(['match_priority', 'confidence'], ascending=[False, False])

    return recommendations_df.head(top_n)


# Exemple d'utilisation du syst√®me de recommandation
print("\nEXEMPLE DE RECOMMANDATION:")
print("-" * 25)

# D√©finition d'un poste exemple
job_example = {
    'job_title': 'Data Scientist',
    'required_experience': 5,
    'education': 'Master',
    'required_skills': 'Python, Machine Learning, Statistics'
}

print(f"Poste √† pourvoir: {job_example['job_title']}")
print(f"Exp√©rience requise: {job_example['required_experience']} ans")
print(f"√âducation requise: {job_example['education']}")
print(f"Comp√©tences requises: {job_example['required_skills']}")

# G√©n√©ration des recommandations
top_candidates = recommend_candidates(job_example, top_n=10)

print(f"\nTop 10 candidats recommand√©s:")
print("=" * 50)
for idx, candidate in top_candidates.iterrows():
    print(f"\n{idx + 1}. {candidate['candidate_name']}")
    print(f"   Exp√©rience: {candidate['experience']} ans")
    print(f"   √âducation: {candidate['education']}")
    print(f"   Comp√©tences: {candidate['skills'][:50]}...")
    print(f"   Match pr√©dit: {candidate['predicted_match']}")
    print(f"   Confiance: {candidate['confidence']:.3f}")
    print(f"   Score r√©el: {candidate['actual_match_score']:.3f}")

# =====================================================
# 9. √âVALUATION ET M√âTRIQUES DE PERFORMANCE
# =====================================================

print("\n\n9. √âVALUATION GLOBALE DU SYST√àME")
print("-" * 35)


# √âvaluation du syst√®me de recommandation
def evaluate_recommendation_system(n_tests=100):
    """
    √âvalue la performance du syst√®me de recommandation
    """
    correct_predictions = 0
    total_predictions = 0

    # Test sur un √©chantillon du dataset
    test_sample = df_clean.sample(min(n_tests, len(df_clean)))

    for idx, row in test_sample.iterrows():
        job_req = {
            'job_title': row['JobTitle'],
            'required_experience': row['RequiredExperience'],
            'education': row['EducationRequired'],
            'required_skills': row['RequiredSkills']
        }

        # Recommandation pour ce poste
        recommendations = recommend_candidates(job_req, top_n=5)

        # V√©rification si le candidat original est dans les recommandations
        if row['CandidateName'] in recommendations['candidate_name'].values:
            correct_predictions += 1
        total_predictions += 1

    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    return accuracy, correct_predictions, total_predictions


# √âvaluation du syst√®me
rec_accuracy, correct, total = evaluate_recommendation_system(50)
print(f"Pr√©cision du syst√®me de recommandation: {rec_accuracy:.3f}")
print(f"Recommandations correctes: {correct}/{total}")

# M√©triques suppl√©mentaires
print(f"\nM√âTRIQUES SUPPL√âMENTAIRES:")
print("-" * 25)

# Distribution des scores de match
match_distribution = df_clean['MatchScore'].describe()
print(f"Distribution des scores de match:")
print(match_distribution)

# Analyse par cat√©gorie d'exp√©rience
exp_analysis = df_clean.groupby(df_clean['ExperienceYears'].apply(categorize_experience)).agg({
    'MatchScore': ['mean', 'std', 'count']
}).round(3)
print(f"\nAnalyse par cat√©gorie d'exp√©rience:")
print(exp_analysis)

# Analyse par niveau d'√©ducation
edu_analysis = df_clean.groupby('EducationLevel')['MatchScore'].agg(['mean', 'count']).sort_values('mean',
                                                                                                   ascending=False).head(
    10)
print(f"\nTop 10 niveaux d'√©ducation par score de match moyen:")
print(edu_analysis.round(3))

# =====================================================
# 10. INTERPR√âTATION ET CONCLUSIONS
# =====================================================

print("\n\n10. INTERPR√âTATION ET CONCLUSIONS")
print("-" * 35)

print("R√âSUM√â DE L'ANALYSE:")
print("=" * 25)

print(f"‚úì Dataset analys√©: {df.shape[0]} candidats, {df.shape[1]} variables")
print(f"‚úì Donn√©es nettoy√©es: {df_clean.shape[0]} enregistrements apr√®s nettoyage")
print(f"‚úì Variables cr√©√©es: 4 nouvelles variables (ExperienceDiff, cat√©gories, encodages)")

if len(frequent_itemsets) > 0:
    print(f"‚úì R√®gles d'association: {len(frequent_itemsets)} itemsets fr√©quents identifi√©s")
else:
    print("‚úì R√®gles d'association: Analyse effectu√©e (donn√©es peu structur√©es pour les r√®gles)")

print(f"‚úì Clustering: {optimal_k} clusters optimaux identifi√©s (K-means)")
print(f"‚úì Classification: Meilleur mod√®le = {best_model_name} (pr√©cision: {results[best_model_name]['accuracy']:.3f})")
print(f"‚úì Syst√®me de recommandation: Pr√©cision = {rec_accuracy:.3f}")

print(f"\nINSIGHTS PRINCIPAUX:")
print("-" * 20)

# Corr√©lations importantes
corr_exp_match = df_clean[['ExperienceYears', 'MatchScore']].corr().iloc[0, 1]
corr_req_match = df_clean[['RequiredExperience', 'MatchScore']].corr().iloc[0, 1]

print(f"‚Ä¢ Corr√©lation exp√©rience-match: {corr_exp_match:.3f}")
print(f"‚Ä¢ Corr√©lation exp√©rience requise-match: {corr_req_match:.3f}")

# Meilleur niveau d'√©ducation
best_education = edu_analysis.index[0]
best_edu_score = edu_analysis.iloc[0]['mean']
print(f"‚Ä¢ Meilleur niveau d'√©ducation: {best_education} (score moyen: {best_edu_score:.3f})")

# Analyse des clusters
print(f"‚Ä¢ {optimal_k} profils de candidats identifi√©s par clustering")

# Variables les plus importantes
if 'Random Forest' in results:
    top_feature = feature_importance.iloc[0]
    print(f"‚Ä¢ Variable la plus importante: {top_feature['Feature']} (importance: {top_feature['Importance']:.3f})")

print(f"\nRECOMMANDATIONS M√âTIER:")
print("-" * 25)

print("1. AM√âLIORATION DU MATCHING:")
print("   ‚Ä¢ Int√©grer davantage de variables de comp√©tences sp√©cifiques")
print("   ‚Ä¢ Pond√©rer diff√©remment selon le type de poste")
print("   ‚Ä¢ Consid√©rer l'exp√©rience sectorielle en plus de l'exp√©rience totale")

print("\n2. OPTIMISATION DU SYST√àME:")
print("   ‚Ä¢ Collecter plus de donn√©es pour am√©liorer les r√®gles d'association")
print("   ‚Ä¢ Impl√©menter un feedback loop pour am√©liorer les pr√©dictions")
print("   ‚Ä¢ Ajouter des crit√®res de localisation et de salaire")

print("\n3. UTILISATION DES CLUSTERS:")
print("   ‚Ä¢ Personnaliser les recommandations par cluster de candidats")
print("   ‚Ä¢ Adapter les crit√®res de matching selon le profil identifi√©")
print("   ‚Ä¢ D√©velopper des strat√©gies de sourcing sp√©cifiques par cluster")

print(f"\nLIMITES DE L'√âTUDE:")
print("-" * 20)
print("‚Ä¢ Donn√©es simul√©es - r√©sultats √† valider sur donn√©es r√©elles")
print("‚Ä¢ Variables de comp√©tences peu structur√©es")
print("‚Ä¢ Manque de donn√©es temporelles pour l'√©volution des candidats")
print("‚Ä¢ Absence de feedback des recruteurs pour validation")

print(f"\nPISTES D'AM√âLIORATION:")
print("-" * 25)
print("‚Ä¢ Int√©gration de NLP pour analyser les comp√©tences textuelles")
print("‚Ä¢ Utilisation de r√©seaux de neurones pour des patterns complexes")
print("‚Ä¢ D√©veloppement d'un syst√®me de scoring multi-crit√®res")
print("‚Ä¢ Impl√©mentation d'un syst√®me de recommandation collaborative")

print("\n" + "=" * 60)
print("FIN DE L'ANALYSE - SYST√àME DE RECOMMANDATION CANDIDAT-EMPLOI")
print("=" * 60)

# Sauvegarde des r√©sultats principaux
results_summary = {
    'dataset_size': df.shape,
    'clean_dataset_size': df_clean.shape,
    'best_model': best_model_name,
    'best_model_accuracy': results[best_model_name]['accuracy'],
    'optimal_clusters': optimal_k,
    'recommendation_accuracy': rec_accuracy,
    'top_correlations': {
        'experience_match': corr_exp_match,
        'required_exp_match': corr_req_match
    }
}

print(f"\nR√©sultats sauvegard√©s dans results_summary")
print("Analyse termin√©e avec succ√®s!")

# Visualisation finale - Dashboard de r√©sultats
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('DASHBOARD - R√âSULTATS DE L\'ANALYSE DATA MINING', fontsize=16, fontweight='bold')

# 1. Distribution des scores de match
axes[0, 0].hist(df_clean['MatchScore'], bins=20, color='skyblue', alpha=0.7, edgecolor='black')
axes[0, 0].set_title('Distribution des Scores de Match')
axes[0, 0].set_xlabel('Score de Match')
axes[0, 0].set_ylabel('Fr√©quence')

# 2. Performance des mod√®les
model_names = list(results.keys())
accuracies = [results[model]['accuracy'] for model in model_names]
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
bars = axes[0, 1].bar(model_names, accuracies, color=colors)
axes[0, 1].set_title('Performance des Mod√®les de Classification')
axes[0, 1].set_ylabel('Pr√©cision')
axes[0, 1].tick_params(axis='x', rotation=45)
for bar, acc in zip(bars, accuracies):
    axes[0, 1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom')

# 3. Clustering visualization
scatter = axes[0, 2].scatter(df_clean['ExperienceYears'], df_clean['MatchScore'],
                             c=df_features['KMeans_Cluster'], cmap='viridis', alpha=0.6)
axes[0, 2].set_title(f'Clustering K-Means ({optimal_k} clusters)')
axes[0, 2].set_xlabel('Exp√©rience (ann√©es)')
axes[0, 2].set_ylabel('Score de Match')

# 4. Corr√©lations
corr_data = df_clean[['ExperienceYears', 'RequiredExperience', 'MatchScore']].corr()
im = axes[1, 0].imshow(corr_data, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
axes[1, 0].set_title('Matrice de Corr√©lation')
axes[1, 0].set_xticks(range(len(corr_data.columns)))
axes[1, 0].set_yticks(range(len(corr_data.columns)))
axes[1, 0].set_xticklabels(corr_data.columns, rotation=45)
axes[1, 0].set_yticklabels(corr_data.columns)
for i in range(len(corr_data.columns)):
    for j in range(len(corr_data.columns)):
        axes[1, 0].text(j, i, f'{corr_data.iloc[i, j]:.2f}', ha='center', va='center')

# 5. Top √©ducations par score
top_edu = edu_analysis.head(5)
axes[1, 1].barh(range(len(top_edu)), top_edu['mean'], color='lightgreen')
axes[1, 1].set_title('Top 5 Niveaux d\'√âducation')
axes[1, 1].set_xlabel('Score de Match Moyen')
axes[1, 1].set_yticks(range(len(top_edu)))
axes[1, 1].set_yticklabels(top_edu.index)

# 6. M√©triques de performance globales
metrics = ['Pr√©cision\nClassification', 'Pr√©cision\nRecommandation', 'Nb Clusters\nOptimal', 'Corr√©lation\nExp-Match']
values = [results[best_model_name]['accuracy'], rec_accuracy, optimal_k / 10,
          abs(corr_exp_match)]  # Normalisation pour le nb de clusters
colors_metrics = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']
bars = axes[1, 2].bar(metrics, values, color=colors_metrics)
axes[1, 2].set_title('M√©triques de Performance Globales')
axes[1, 2].set_ylabel('Valeur')
for bar, val, metric in zip(bars, values, metrics):
    if 'Clusters' in metric:
        axes[1, 2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                        f'{optimal_k}', ha='center', va='bottom')
    else:
        axes[1, 2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                        f'{val:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()

print("\nüéâ ANALYSE COMPL√àTE TERMIN√âE AVEC SUCC√àS! üéâ")